# Changelog

Todos los cambios notables en SentimentInsightUAM se documentar√°n en este archivo.

El formato est√° basado en [Keep a Changelog](https://keepachangelog.com/es-ES/1.0.0/),
y este proyecto adhiere a [Versionado Sem√°ntico](https://semver.org/lang/es/).

## Gu√≠a para Contribuidores y Agentes

Este CHANGELOG documenta:
- ‚úÖ **Caracter√≠sticas implementadas**: Funcionalidades completamente operativas
- üöß **En desarrollo**: Caracter√≠sticas parcialmente implementadas
- üìã **Planificadas**: Pr√≥ximas caracter√≠sticas seg√∫n roadmap
- üêõ **Correcciones**: Bugs resueltos y mejoras
- üîß **Cambios t√©cnicos**: Refactorizaciones y optimizaciones

### Convenci√≥n de Commits Relacionados
- `feat:` - Nueva caracter√≠stica
- `fix:` - Correcci√≥n de bug
- `refactor:` - Refactorizaci√≥n sin cambio de funcionalidad
- `perf:` - Mejora de rendimiento
- `docs:` - Documentaci√≥n
- `test:` - Tests
- `chore:` - Tareas de mantenimiento

---

## [Unreleased]

### ‚ú® Added
- **Sistema de Documentaci√≥n Completa para Desarrollo**
  - `CHANGELOG.md` - Historial completo de versiones con gu√≠a para contribuidores
  - `.github/copilot-instructions.md` - Contexto permanente para GitHub Copilot
  - `.github/COMMIT_CONVENTION.md` - Convenci√≥n de commits y versionado sem√°ntico

### üìã Planificado
- Persistencia en PostgreSQL para datos estructurados
- Persistencia en MongoDB para opiniones textuales
- An√°lisis de sentimiento con modelo BERT
- API REST con FastAPI
- Sistema de jobs programados con APScheduler
- Dashboard de visualizaci√≥n de datos
- Tests unitarios y de integraci√≥n

---

## [1.0.0] - 2024-11-08

### ‚ú® Caracter√≠sticas Principales Implementadas

#### üéØ Sistema de Scraping Completo
- **Extracci√≥n de Directorio UAM** (`src/uam/nombres_uam.py`)
  - Scraping del [Directorio Oficial UAM Azcapotzalco](https://sistemas.azc.uam.mx/Somos/Directorio/)
  - Carga din√°mica completa mediante clics en "Ver m√°s Profesorado"
  - Normalizaci√≥n de nombres con `slugify`
  - Extracci√≥n de 150+ profesores del Departamento de Sistemas
  - Salida en formato JSON con estructura `{name, slug, url}`

- **Scraping de Perfiles MisProfesores.com** (`src/mp/scrape_prof.py`)
  - B√∫squeda normalizada sin acentos (case-insensitive)
  - Navegaci√≥n directa por href (evita problemas de scroll/clic)
  - Extracci√≥n completa de perfil: calificaciones, etiquetas, rese√±as
  - Paginaci√≥n autom√°tica sin l√≠mite artificial
  - Reintentos con backoff exponencial v√≠a `tenacity` (hasta 4 intentos)
  - Delays inteligentes entre requests (2-4s variables)
  - Timeouts configurados: 45s navegaci√≥n, 30s selectores

#### üíæ Sistema de Cach√© Inteligente
- **Detecci√≥n Autom√°tica de Cambios**
  - Compara n√∫mero de rese√±as: cach√© vs actual
  - Tolerancia de ¬±5 rese√±as para evitar re-scraping innecesario
  - Solo actualiza cuando detecta cambios reales
  - Opci√≥n `force=True` para forzar actualizaci√≥n

- **Persistencia Dual**
  - **HTML Original**: `data/outputs/html/{slug}.html` (auditor√≠a)
  - **JSON Estructurado**: `data/outputs/profesores/{slug}.json` (consumo)
  - Ventajas:
    - Re-parsing offline sin re-scraping
    - Debugging facilitado
    - An√°lisis hist√≥rico
    - Consumo directo por aplicaciones

#### üîç Parser HTML Robusto (`src/mp/parser.py`)
- **Extracci√≥n de Perfil**
  - Calificaci√≥n general (`overall_quality`)
  - Dificultad (`difficulty`)
  - Porcentaje de recomendaci√≥n (`recommend_percent`)
  - Etiquetas con contadores (ej: `EXCELENTE CLASE (25)`)

- **Extracci√≥n de Rese√±as**
  - Fecha (convertida a ISO 8601: YYYY-MM-DD)
  - Curso
  - Calificaciones (general, facilidad)
  - Asistencia (Obligatoria/No obligatoria)
  - Calificaci√≥n recibida (10, MB, B, etc.)
  - Nivel de inter√©s (Alta, Media, Baja)
  - Etiquetas de la rese√±a
  - Comentario textual completo

- **Conteo de P√°ginas**
  - M√©todo principal: Contador total / 5 rese√±as por p√°gina
  - Fallback: N√∫mero m√°ximo en botones de paginaci√≥n
  - Retorna m√≠nimo 1 p√°gina

#### üñ•Ô∏è CLI Interactivo (`src/cli.py`)

**Comandos Disponibles:**

1. **`nombres-uam`** - Extracci√≥n de profesores UAM
   ```bash
   python -m src.cli nombres-uam
   ```
   - Extrae lista completa de profesores del directorio
   - Salida JSON a stdout (puede redirigirse a archivo)
   - Genera `data/inputs/profesor_nombres.json` autom√°ticamente

2. **`prof`** - Scraping individual (Modo Interactivo)
   ```bash
   python -m src.cli prof
   ```
   - Carga lista de `profesor_nombres.json`
   - Si no existe, obtiene autom√°ticamente de UAM
   - Muestra men√∫ numerado en 4 columnas
   - Selecci√≥n por n√∫mero
   - Scraping con cach√© inteligente
   - Muestra resumen al finalizar

3. **`prof --name`** - Scraping directo
   ```bash
   python -m src.cli prof --name "Juan P√©rez Garc√≠a"
   ```
   - B√∫squeda directa sin men√∫
   - Ideal para automatizaci√≥n y scripts

4. **`scrape-all`** - Scraping masivo automatizado ‚≠ê
   ```bash
   python -m src.cli scrape-all
   ```
   - **Procesamiento secuencial** de todos los profesores
   - **Cach√© inteligente** por profesor individual
   - **Detecci√≥n autom√°tica de cambios** (evita scraping redundante)
   - **Rate limiting**: Delays variables 2-4s entre profesores
   - **Progreso en tiempo real**: Contador `[n/total]`
   - **Manejo robusto de errores**: Contin√∫a si un profesor falla
   - **Resumen final** con estad√≠sticas:
     - Total procesados
     - Scrapeados exitosamente
     - Obtenidos de cach√©
     - Errores
   
   **Ejemplo de salida:**
   ```
   Iniciando scraping de 150 profesores...
   ================================================================================
   
   [1/150] Procesando: Juan Perez Garcia
     -> Scrapeado exitosamente (47 rese√±as)
     -> Esperando 2s antes del siguiente...
   
   [2/150] Procesando: Maria Lopez Hernandez
     -> Cache vigente (32 rese√±as)
     -> Esperando 4s antes del siguiente...
   
   [3/150] Procesando: Carlos Rodriguez Torres
     -> Detectados cambios: 28 ‚Üí ~35 rese√±as
     -> Scrapeado exitosamente (35 rese√±as)
   ...
   
   ================================================================================
   RESUMEN DE SCRAPING
   ================================================================================
   Total profesores procesados: 150
   Scrapeados exitosamente: 28
   Obtenidos de cache: 119
   Errores: 3
   ================================================================================
   ```

#### üåê Context Manager de Navegador (`src/core/browser.py`)
- Playwright con Chromium
- User agent realista (Chrome 122)
- Modo headless configurable via `.env`
- Gesti√≥n autom√°tica de ciclo de vida
- Pattern async context manager

### üêõ Correcciones Implementadas

#### Fix: AttributeError en Parser (v0.9.1 ‚Üí v1.0.0)
- **Problema**: `select_one()` retornaba `None`, causando error al llamar `.get_text()`
- **Causa ra√≠z**: Patr√≥n `(elem or "").get_text()` fallaba porque `""` no tiene m√©todo `.get_text()`
- **Soluci√≥n**: Pattern correcto en 7 ubicaciones:
  ```python
  # ‚ùå Antes
  course = (td_c.select_one(".name .response") or "").get_text(strip=True)
  
  # ‚úÖ Despu√©s
  course_elem = td_c.select_one(".name .response")
  course = course_elem.get_text(strip=True) if course_elem else None
  ```
- **Ubicaciones corregidas**:
  - `parser.py::parse_reviews()`: course, attendance, grade_received, interest, comment_elem
  - `parser.py::parse_profile()`: name element extraction
  - Total: 7 fixes aplicados

#### Perf: B√∫squeda Mejorada - Navegaci√≥n por href
- **Problema inicial**: Timeouts frecuentes con b√∫squeda por clic
- **Evoluci√≥n**:
  1. **v0.1**: B√∫squeda simple + clic ‚Üí Timeouts
  2. **v0.5**: A√±adido Enter + `wait_for_load_state("networkidle")` ‚Üí Lento
  3. **v1.0**: Navegaci√≥n directa por href ‚Üí ‚úÖ √ìptimo
- **Ventajas actuales**:
  - Sin problemas de scroll/viewport
  - No depende de `networkidle` (m√°s r√°pido)
  - Esperas expl√≠citas de selectores cr√≠ticos
  - Matching normalizado sin acentos

#### Perf: Paginaci√≥n Optimizada
- **Antes**: Limitado a 9 p√°ginas por clic en botones
- **Ahora**: Sin l√≠mite, navegaci√≥n directa por URL
  ```python
  # Profesores con 50+ p√°ginas de rese√±as ahora soportados
  for p in range(1, pages + 1):
      url = profile_url if p == 1 else f"{profile_url}?pag={p}"
  ```

### üîß Arquitectura del Proyecto

```
SentimentInsightUAM/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                 # CLI con 3 comandos
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ browser.py         # Context manager Playwright
‚îÇ   ‚îú‚îÄ‚îÄ uam/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nombres_uam.py     # Scraper directorio UAM
‚îÇ   ‚îî‚îÄ‚îÄ mp/
‚îÇ       ‚îú‚îÄ‚îÄ parser.py          # Parser HTML especializado
‚îÇ       ‚îî‚îÄ‚îÄ scrape_prof.py     # Scraper con cach√© inteligente
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ inputs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ profesor_nombres.json  # Lista de profesores UAM
‚îÇ   ‚îî‚îÄ‚îÄ outputs/
‚îÇ       ‚îú‚îÄ‚îÄ html/              # HTML original (auditor√≠a)
‚îÇ       ‚îî‚îÄ‚îÄ profesores/        # JSON estructurado (consumo)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ TECHNICAL_DOCUMENTATION.md  # Documentaci√≥n t√©cnica completa
‚îú‚îÄ‚îÄ requirements.txt           # Dependencias Python
‚îú‚îÄ‚îÄ README.md                  # Documentaci√≥n usuario
‚îú‚îÄ‚îÄ CHANGELOG.md               # Este archivo
‚îî‚îÄ‚îÄ .env                       # Configuraci√≥n (opcional)
```

### üì¶ Dependencias Principales
```
playwright>=1.46           # Navegaci√≥n automatizada
beautifulsoup4>=4.12       # Parsing HTML
lxml>=5.2                  # Parser XML/HTML r√°pido
pydantic>=2.9              # Validaci√≥n de datos (futuro)
python-slugify>=8.0        # Normalizaci√≥n de nombres
tenacity>=9.0              # Reintentos con backoff
python-dotenv>=1.0         # Variables de entorno
```

### üîí Variables de Entorno
```env
HEADLESS=true              # Modo headless del navegador (true/false)
```

### üìä M√©tricas de Rendimiento
- **Tiempo promedio por profesor**: ~5-8 segundos (dependiendo de p√°ginas)
- **Scraping completo (150 profesores)**: ~15-20 minutos con cach√©
- **Tasa de √©xito**: >95% con reintentos autom√°ticos
- **Uso de cach√©**: ~80% en ejecuciones subsecuentes

### üéØ Pr√≥ximos Pasos (Roadmap v2.0.0)

#### Fase 1: Persistencia en Bases de Datos
- [ ] Esquema PostgreSQL para datos estructurados
  - Tablas: `profesores`, `perfiles`, `resenias_metadata`, `etiquetas`, `cursos`
  - Relaciones many-to-many para etiquetas
  - Historial de scraping para auditor√≠a
- [ ] Esquema MongoDB para opiniones textuales
  - Colecci√≥n `opiniones` con campo `sentimiento`
  - √çndices full-text para b√∫squeda
  - Referencia a PostgreSQL via `mongo_opinion_id`

#### Fase 2: An√°lisis de Sentimiento
- [ ] Integraci√≥n de modelo BERT en espa√±ol
- [ ] Worker para procesamiento as√≠ncrono de opiniones
- [ ] An√°lisis por aspectos (explicaci√≥n, disponibilidad, evaluaci√≥n)
- [ ] Puntuaci√≥n de sentimiento (-1 a 1)
- [ ] Clasificaci√≥n (positivo/neutral/negativo)

#### Fase 3: API REST
- [ ] FastAPI con documentaci√≥n OpenAPI autom√°tica
- [ ] Endpoints para consulta de profesores, rese√±as, estad√≠sticas
- [ ] Autenticaci√≥n JWT (opcional)
- [ ] Paginaci√≥n en todos los listados
- [ ] Filtros avanzados (fecha, curso, calificaci√≥n)
- [ ] Cach√© con Redis para consultas frecuentes

#### Fase 4: Sistema de Jobs
- [ ] APScheduler para jobs programados
- [ ] Job incremental cada 6 horas
- [ ] Job nocturno masivo (2:00 AM)
- [ ] Job de an√°lisis BERT cada hora
- [ ] Job de mantenimiento semanal
- [ ] Monitoreo y alertas

#### Fase 5: Frontend
- [ ] Dashboard de visualizaci√≥n con React/Vue
- [ ] Gr√°ficas de tendencias temporales
- [ ] Comparaci√≥n entre profesores
- [ ] B√∫squeda avanzada
- [ ] Filtros por departamento, materia, calificaci√≥n

---

## Notas para Desarrolladores/Agentes

### ü§ñ Para Agentes que Implementen Nuevas Features

**Antes de implementar una nueva caracter√≠stica:**
1. ‚úÖ Lee este CHANGELOG para entender el estado actual
2. ‚úÖ Revisa `docs/TECHNICAL_DOCUMENTATION.md` para arquitectura propuesta
3. ‚úÖ Verifica que la feature no est√© ya implementada
4. ‚úÖ Actualiza este archivo con tus cambios en la secci√≥n `[Unreleased]`
5. ‚úÖ Sigue las convenciones de commit establecidas

**Al finalizar la implementaci√≥n:**
1. ‚úÖ Mueve la feature de `[Unreleased]` a una nueva versi√≥n
2. ‚úÖ Actualiza la fecha de la versi√≥n
3. ‚úÖ Documenta breaking changes si los hay
4. ‚úÖ Actualiza README.md si afecta el uso

### üë®‚Äçüíª Para Desarrolladores Nuevos

**Puntos de entrada recomendados:**

1. **Para entender el scraping**: 
   - `src/mp/scrape_prof.py::find_and_scrape()` - Funci√≥n principal
   - `src/mp/parser.py` - L√≥gica de extracci√≥n

2. **Para entender el cach√©**:
   - `src/mp/scrape_prof.py::_get_cached_data()` - Lectura de cach√©
   - `src/mp/scrape_prof.py::find_and_scrape()` - L√≥gica de detecci√≥n de cambios

3. **Para entender la CLI**:
   - `src/cli.py::main()` - Entry point
   - `src/cli.py::scrape_all_professors()` - Comando masivo

4. **Para agregar nueva fuente de datos**:
   - Usar `src/core/browser.py` como base
   - Seguir patr√≥n de `src/uam/nombres_uam.py`
   - Implementar parser especializado

### üõ°Ô∏è Buenas Pr√°cticas Establecidas

1. **Cach√© siempre que sea posible**: Evita re-scraping innecesario
2. **Persistencia dual**: HTML + JSON para m√°xima flexibilidad
3. **Normalizaci√≥n de texto**: Usar `slugify` para nombres de archivo
4. **Manejo de errores**: Try-except con logging claro
5. **Rate limiting**: Delays entre requests (2-4s variable)
6. **Reintentos**: Usar `tenacity` para operaciones de red
7. **Timeouts expl√≠citos**: 45s navegaci√≥n, 30s selectores
8. **Async/await**: Todo el c√≥digo de I/O es as√≠ncrono

### üìù Template para Agregar Nuevas Versiones

```markdown
## [X.Y.Z] - YYYY-MM-DD

### Added
- Nueva caracter√≠stica A
- Nueva caracter√≠stica B

### Changed
- Cambio en caracter√≠stica existente C
- Refactorizaci√≥n de m√≥dulo D

### Deprecated
- Caracter√≠stica E ser√° removida en v(X+1).0.0

### Removed
- Caracter√≠stica F removida

### Fixed
- Bug #123: Descripci√≥n del fix
- Bug #456: Descripci√≥n del fix

### Security
- Parche de seguridad para vulnerabilidad X
```

---

## Historial de Versiones

### [1.0.0] - 2024-11-08
- ‚úÖ Lanzamiento inicial con scraping completo
- ‚úÖ Sistema de cach√© inteligente
- ‚úÖ CLI con 3 comandos funcionales
- ‚úÖ Persistencia dual (HTML + JSON)
- ‚úÖ Documentaci√≥n completa

---

**√öltima actualizaci√≥n**: 2024-11-08  
**Mantenedores**: Equipo SentimentInsightUAM - UAM Azcapotzalco  
**Licencia**: Open Source (Fines Educativos)
