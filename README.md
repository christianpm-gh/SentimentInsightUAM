# SentimentInsightUAM

Sistema de scraping y anÃ¡lisis de reseÃ±as de profesores de la Universidad AutÃ³noma Metropolitana, Unidad Azcapotzalco. Este proyecto extrae informaciÃ³n del directorio oficial de la UAM y de perfiles en MisProfesores.com para anÃ¡lisis de sentimiento y visualizaciÃ³n de datos.

## ğŸ¯ CaracterÃ­sticas

- **ExtracciÃ³n de nombres**: Obtiene automÃ¡ticamente la lista de profesores del [Directorio UAM Azcapotzalco](https://sistemas.azc.uam.mx/Somos/Directorio/)
- **Scraping robusto**: Navega y extrae perfiles completos de MisProfesores.com con:
  - BÃºsqueda normalizada (sin acentos, case-insensitive)
  - NavegaciÃ³n directa por URL para evitar timeouts
  - PaginaciÃ³n automÃ¡tica de reseÃ±as
  - Reintentos con backoff exponencial
- **Scraping masivo**: Comando `scrape-all` para procesar todos los profesores automÃ¡ticamente
  - Procesamiento secuencial con delays inteligentes
  - DetecciÃ³n automÃ¡tica de cambios por profesor
  - Resumen de progreso en tiempo real
  - Manejo robusto de errores sin interrumpir el proceso
- **CachÃ© inteligente**: 
  - Detecta automÃ¡ticamente si un profesor ya fue scrapeado
  - Compara nÃºmero de reseÃ±as para detectar cambios
  - Evita re-scraping innecesario (eficiencia y respeto al servidor)
  - Permite forzar actualizaciÃ³n cuando sea necesario
- **Persistencia dual**:
  - HTML original guardado en `data/outputs/html/` (auditorÃ­a)
  - JSON estructurado en `data/outputs/profesores/` (consumo)
  - Nombres normalizados con slugify para consistencia
- **Parsing estructurado**: Extrae calificaciones, etiquetas, comentarios y metadatos
- **CLI interactivo**: Interfaz de lÃ­nea de comandos con menÃº de selecciÃ³n

## ğŸ“‹ Requisitos

### OpciÃ³n A: Con Docker (Recomendado)

- **Docker** >= 20.10
- **Docker Compose** >= 2.0
- Python 3.11+ (solo para el scraper)

### OpciÃ³n B: Sin Docker

- Python 3.11+
- PostgreSQL >= 15.0
- MongoDB >= 7.0
- Playwright (Chromium)
- BeautifulSoup4
- Dependencias listadas en `requirements.txt`

## ğŸš€ InstalaciÃ³n

### OpciÃ³n A: Con Docker (Recomendado para Desarrollo)

Esta opciÃ³n configura automÃ¡ticamente las bases de datos PostgreSQL y MongoDB en contenedores aislados.

#### 1. Clonar el repositorio

```bash
git clone https://github.com/christianpm-gh/SentimentInsightUAM.git
cd SentimentInsightUAM
```

#### 2. Configurar variables de entorno

```bash
# Copiar archivo de configuraciÃ³n para Docker
cp .env.docker .env

# (Opcional) Editar contraseÃ±as para producciÃ³n
nano .env
```

#### 3. Iniciar bases de datos con Docker

```bash
# OpciÃ³n 1: Con Makefile (mÃ¡s conveniente)
make docker-up

# OpciÃ³n 2: Docker Compose directo
docker-compose up -d
```

Esto iniciarÃ¡:
- âœ… PostgreSQL 15 en puerto 5432
- âœ… MongoDB 7.0 en puerto 27017
- âœ… Scripts de inicializaciÃ³n ejecutados automÃ¡ticamente
- âœ… 8 tablas PostgreSQL creadas
- âœ… 2 colecciones MongoDB creadas
- âœ… 21 etiquetas iniciales insertadas

#### 4. Instalar dependencias de Python

```bash
# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o: .venv\Scripts\activate  # Windows

# Instalar dependencias
make install
# o manualmente:
pip install -r requirements.txt
python -m playwright install chromium
```

#### 5. Verificar configuraciÃ³n

```bash
# Verificar estado de bases de datos
make db-status

# Conectar a PostgreSQL
make db-psql

# Conectar a MongoDB
make db-mongo
```

**Â¡Listo!** Las bases de datos estÃ¡n configuradas y listas para usar.

**Comandos Ãºtiles con Docker:**

```bash
make help              # Ver todos los comandos disponibles
make docker-up         # Iniciar contenedores
make docker-down       # Detener contenedores
make docker-logs       # Ver logs en tiempo real
make db-status         # Verificar estado de bases de datos
make db-reset          # Reiniciar bases de datos (DESTRUYE DATOS)
```

**DocumentaciÃ³n completa:** Ver [docs/DOCKER_SETUP.md](docs/DOCKER_SETUP.md)

---

### OpciÃ³n B: InstalaciÃ³n Manual (Sin Docker)

Para instalaciÃ³n manual de PostgreSQL y MongoDB, consulta la guÃ­a completa en [docs/DATABASE_SETUP.md](docs/DATABASE_SETUP.md).

#### 1. Clonar el repositorio

```bash
git clone https://github.com/christianpm-gh/SentimentInsightUAM.git
cd SentimentInsightUAM
```

#### 2. Crear entorno virtual

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

#### 3. Instalar dependencias

```bash
pip install -r requirements.txt
python -m playwright install chromium
```

#### 4. Configurar bases de datos

Sigue la guÃ­a detallada en [docs/DATABASE_SETUP.md](docs/DATABASE_SETUP.md) para:
- Instalar PostgreSQL 15+
- Instalar MongoDB 7.0+
- Ejecutar scripts de inicializaciÃ³n
- Configurar usuarios y permisos

#### 5. Configurar variables de entorno

```bash
# Crear archivo .env con tus credenciales
nano .env
```

Ver ejemplo en `.env.docker` para la estructura requerida.

## ğŸ’» Uso

### 1. Extraer nombres de profesores UAM

Obtiene la lista completa de profesores del Departamento de Sistemas de la UAM Azcapotzalco:

```bash
python -m src.cli nombres-uam
```

Esto generarÃ¡ automÃ¡ticamente `data/inputs/profesor_nombres.json` con la informaciÃ³n de todos los profesores.

### 2. Scrapear perfil de un profesor

**Modo interactivo** (recomendado):
```bash
python -m src.cli prof
```
Se mostrarÃ¡ un menÃº numerado con todos los profesores disponibles.

**Modo directo** (por nombre):
```bash
python -m src.cli prof --name "Juan PÃ©rez GarcÃ­a"
```

### 3. Scrapear todos los profesores

Procesa automÃ¡ticamente todos los profesores del directorio UAM con cachÃ© inteligente:

```bash
python -m src.cli scrape-all
```

**CaracterÃ­sticas del scraping masivo:**

- Procesa todos los profesores secuencialmente
- Aplica delays de 2-4 segundos entre profesores para evitar bloqueos
- Detecta automÃ¡ticamente si un profesor necesita actualizaciÃ³n
- Solo re-scrapea cuando hay cambios en el nÃºmero de reseÃ±as
- Muestra progreso en tiempo real con contador
- Maneja errores de forma individual sin detener el proceso completo
- Genera resumen final con estadÃ­sticas

**Salida ejemplo:**

```
Iniciando scraping de 150 profesores...
================================================================================

[1/150] Procesando: Juan Perez Garcia
  -> Scrapeado exitosamente (47 reseÃ±as)
  -> Esperando 2s antes del siguiente...

[2/150] Procesando: Maria Lopez Hernandez
  -> Cache vigente (32 reseÃ±as)
  -> Esperando 4s antes del siguiente...

[3/150] Procesando: Carlos Rodriguez Torres
  -> Detectados cambios: 28 -> ~35 reseÃ±as
  -> Scrapeado exitosamente (35 reseÃ±as)
  -> Esperando 2s antes del siguiente...

...

================================================================================
RESUMEN DE SCRAPING
================================================================================
Total profesores procesados: 150
Scrapeados exitosamente: 28
Obtenidos de cache: 119
Errores: 3
================================================================================
```

**PrevenciÃ³n de bloqueos:**

- Delays variables (no detectables como patrÃ³n automÃ¡tico)
- User agent realista configurado en el navegador
- Reintentos automÃ¡ticos con backoff exponencial (via tenacity)
- Timeouts apropiados para cada operaciÃ³n
- Respeto a los lÃ­mites del servidor

### 4. Salida de datos

El scraper implementa **persistencia automÃ¡tica** con dos formatos:

#### JSON estructurado (`data/outputs/profesores/nombre-profesor.json`)

```json
{
  "name": "Nombre del Profesor",
  "overall_quality": 9.5,
  "difficulty": 7.2,
  "recommend_percent": 95.0,
  "cached": false,
  "tags": [
    {"label": "EXCELENTE CLASE", "count": 25},
    {"label": "INSPIRA", "count": 18}
  ],
  "reviews": [
    {
      "date": "2024-01-15",
      "course": "Estructura de Datos",
      "overall": 10.0,
      "ease": 8.0,
      "attendance": "Obligatoria",
      "grade_received": "10",
      "interest": "Alta",
      "tags": ["BUENA ONDA", "ACCESIBLE"],
      "comment": "Excelente profesor, explica muy bien..."
    }
  ]
}
```

#### HTML original (`data/outputs/html/nombre-profesor.html`)

- Guardado para auditorÃ­a y anÃ¡lisis offline
- Permite re-parsing sin re-scraping
- Ãštil para debugging y mejora del parser

#### CachÃ© Inteligente

El sistema detecta automÃ¡ticamente si un profesor ya fue scrapeado:

```bash
# Primera vez: scraping completo
python -m src.cli prof --name "Juan PÃ©rez"
# âš™ Scrapeando Juan PÃ©rez (9 pÃ¡ginas)...
# âœ“ Guardado: HTML en juan-perez.html, JSON en juan-perez.json
# âœ“ Total reseÃ±as extraÃ­das: 43

# Segunda vez (sin cambios): usa cachÃ©
python -m src.cli prof --name "Juan PÃ©rez"
# âœ“ CachÃ© vigente para Juan PÃ©rez (43 reseÃ±as)
# Fuente: CachÃ©

# Si hay nuevas reseÃ±as: scraping automÃ¡tico
# âœ“ Detectados cambios para Juan PÃ©rez: 43 â†’ ~48 reseÃ±as
```

**Ventajas del cachÃ©:**
- âš¡ Respuesta instantÃ¡nea para profesores ya scrapeados
- ğŸŒ Reduce carga en servidores externos
- â™»ï¸ Evita scraping redundante
- ğŸ¯ Solo actualiza cuando detecta cambios

## ğŸ—ï¸ Arquitectura del Proyecto

```
SentimentInsightUAM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py           # Paquete principal
â”‚   â”œâ”€â”€ cli.py                # Interfaz de lÃ­nea de comandos
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ browser.py        # Context manager de Playwright
â”‚   â”œâ”€â”€ uam/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ nombres_uam.py    # Scraper del directorio UAM
â”‚   â””â”€â”€ mp/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ parser.py         # Parser HTML de MisProfesores
â”‚       â””â”€â”€ scrape_prof.py    # Scraper con cachÃ© inteligente
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ inputs/               # Listas de profesores
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ html/             # HTML original (auditorÃ­a)
â”‚       â””â”€â”€ profesores/       # JSONs estructurados
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ TECHNICAL_DOCUMENTATION.md  # DocumentaciÃ³n tÃ©cnica completa
â”‚   â”œâ”€â”€ DATABASE_DESIGN.md          # DiseÃ±o de bases de datos
â”‚   â”œâ”€â”€ DATABASE_SETUP.md           # ConfiguraciÃ³n manual de BD
â”‚   â””â”€â”€ DOCKER_SETUP.md             # ConfiguraciÃ³n con Docker
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_postgres.sql     # InicializaciÃ³n PostgreSQL
â”‚   â”œâ”€â”€ init_mongo.js         # InicializaciÃ³n MongoDB
â”‚   â””â”€â”€ setup_mongo_user.sh   # Setup de usuario MongoDB
â”œâ”€â”€ docker-compose.yml        # OrquestaciÃ³n de contenedores
â”œâ”€â”€ Makefile                  # Comandos Ãºtiles
â”œâ”€â”€ .env.docker               # Template de variables de entorno
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                      # Variables de entorno (local)
â””â”€â”€ README.md
```
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ nombres_uam.py    # Scraper del directorio UAM
â”‚   â””â”€â”€ mp/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ parser.py         # Parser HTML de MisProfesores
â”‚       â””â”€â”€ scrape_prof.py    # Scraper con cachÃ© inteligente
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ inputs/               # Listas de profesores
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ html/             # HTML original (auditorÃ­a)
â”‚       â””â”€â”€ profesores/       # JSONs estructurados
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ TECHNICAL_DOCUMENTATION.md  # DocumentaciÃ³n tÃ©cnica completa
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                      # Variables de entorno (opcional)
â””â”€â”€ README.md
```

## ğŸ”§ MÃ³dulos Principales

### `src.uam.nombres_uam`

Extrae nombres de profesores del directorio oficial UAM usando:

- Playwright para navegaciÃ³n dinÃ¡mica
- Clic automÃ¡tico en "Ver mÃ¡s Profesorado"
- NormalizaciÃ³n de nombres con slugify

### `src.mp.parser`

Parser HTML especializado que extrae:

- Calificaciones (calidad, dificultad, recomendaciÃ³n)
- Etiquetas con contadores
- ReseÃ±as completas con metadatos
- Conteo automÃ¡tico de pÃ¡ginas

### `src.mp.scrape_prof`

Scraper robusto con:

- BÃºsqueda normalizada (elimina acentos)
- NavegaciÃ³n directa por href
- Esperas explÃ­citas de selectores
- PaginaciÃ³n automÃ¡tica por URL
- Manejo de errores con reintentos
- **CachÃ© inteligente**: Detecta cambios en reseÃ±as
- **Persistencia dual**: HTML + JSON
- **Eficiencia**: Evita re-scraping innecesario

### `src.cli`

CLI con tres comandos principales:

- `nombres-uam`: Extrae lista de profesores del directorio UAM
- `prof`: Scrapea perfil individual (interactivo o directo)
- `scrape-all`: Scrapea todos los profesores con cachÃ© inteligente

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (`.env`)

#### Con Docker

El archivo `.env.docker` contiene todas las configuraciones necesarias:

```env
# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=sentiment_uam_db
POSTGRES_USER=sentiment_admin
POSTGRES_PASSWORD=dev_password_2024

# MongoDB
MONGO_HOST=localhost
MONGO_PORT=27017
MONGO_DB=sentiment_uam_nlp
MONGO_USER=sentiment_admin
MONGO_PASSWORD=dev_password_2024

# URLs de conexiÃ³n
DATABASE_URL=postgresql+asyncpg://sentiment_admin:dev_password_2024@localhost:5432/sentiment_uam_db
MONGO_URL=mongodb://sentiment_admin:dev_password_2024@localhost:27017/sentiment_uam_nlp?authSource=sentiment_uam_nlp

# Scraper
HEADLESS=true
RATE_MIN_MS=400
RATE_MAX_MS=1200
```

#### Sin Docker (InstalaciÃ³n Manual)

Crea un archivo `.env` con tus credenciales personalizadas. Ver [docs/DATABASE_SETUP.md](docs/DATABASE_SETUP.md) para mÃ¡s detalles.

### Comandos con Docker

```bash
# Ver ayuda completa
make help

# GestiÃ³n de contenedores
make docker-up         # Iniciar bases de datos
make docker-down       # Detener bases de datos
make docker-restart    # Reiniciar bases de datos
make docker-logs       # Ver logs en tiempo real

# GestiÃ³n de bases de datos
make db-status         # Verificar estado
make db-psql           # Conectar a PostgreSQL
make db-mongo          # Conectar a MongoDB
make db-reset          # Reiniciar (DESTRUYE DATOS)

# Desarrollo
make install           # Instalar dependencias Python
```

## ğŸ“ Notas Importantes

- **Uso responsable**: Este scraper es para fines educativos. Respeta los TÃ©rminos de Servicio de los sitios web.
- **Rate limiting**: El cÃ³digo incluye delays aleatorios para evitar sobrecarga de servidores.
- **CachÃ© automÃ¡tico**: El sistema detecta automÃ¡ticamente si un profesor ya fue scrapeado y evita scraping redundante.
- **Persistencia**: Todos los datos se guardan en disco automÃ¡ticamente (HTML + JSON).
- **Bases de datos**: PostgreSQL para datos estructurados, MongoDB para anÃ¡lisis de sentimiento (v1.1.0+).
- **Docker**: ConfiguraciÃ³n con contenedores para desarrollo rÃ¡pido y reproducible (v1.1.1+).
- **Timeouts**: Los timeouts estÃ¡n configurados para 45 segundos en navegaciÃ³n y 30 segundos en selectores.
- **PrÃ³ximas caracterÃ­sticas**: API REST con FastAPI, anÃ¡lisis de sentimiento con BERT, dashboard de visualizaciÃ³n.

## ğŸ”® PrÃ³ximas CaracterÃ­sticas

- [ ] Persistencia en PostgreSQL (datos estructurados)
- [ ] Persistencia en MongoDB (comentarios/opiniones)
- [ ] AnÃ¡lisis de sentimiento con BERT
- [ ] API REST para consulta de datos
- [ ] Jobs programados con scheduler
- [ ] Dashboard de visualizaciÃ³n

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, consulta la documentaciÃ³n en `.github/`:

- **[CONTRIBUTING.md](.github/CONTRIBUTING.md)** - GuÃ­a completa de contribuciÃ³n
- **[COMMIT_CONVENTION.md](.github/COMMIT_CONVENTION.md)** - ConvenciÃ³n de mensajes de commit
- **[BRANCH_NAMING.md](.github/BRANCH_NAMING.md)** - ConvenciÃ³n de nombres de ramas
- **[PULL_REQUEST_TEMPLATE.md](.github/PULL_REQUEST_TEMPLATE.md)** - Plantilla para PRs

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto para fines educativos.

## ğŸ‘¥ Equipo

Desarrollado por el equipo de SentimentInsightUAM - UAM Azcapotzalco

---

**âš ï¸ Disclaimer**: Este proyecto es con fines educativos y de investigaciÃ³n. El uso debe cumplir con los TÃ©rminos de Servicio de los sitios web utilizados.
